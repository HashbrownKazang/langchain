{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f331037f-be3f-4782-856f-d55dab952488",
   "metadata": {},
   "source": [
    "# How to migrate chains to LCEL\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [LangChain Expression Language](/docs/concepts#langchain-expression-language-lcel)\n",
    "\n",
    ":::\n",
    "\n",
    "LCEL is designed to streamline the process of building useful apps with LLMs and combining related components. It does this by providing:\n",
    "\n",
    "1. **A unified interface**: Every LCEL object implements the `Runnable` interface, which defines a common set of invocation methods (`invoke`, `batch`, `stream`, `ainvoke`, ...). This makes it possible to also automatically and consistently support useful operations like streaming of intermediate steps and batching, since every chain composed of LCEL objects is itself an LCEL object.\n",
    "2. **Composition primitives**: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internals, and more.\n",
    "\n",
    "LangChain maintains a number of legacy abstractions. Many of these can be reimplemented via short combinations of LCEL primitives. Doing so confers some advantages:\n",
    "\n",
    "- The resulting chains typically implement the full `Runnable` interface, including streaming and asynchronous support where appropriate;\n",
    "- The chains may be more easily extended or modified;\n",
    "- The parameters of the chain are typically surfaced for easier customization (e.g., prompts) over previous versions, which tended to be subclasses and had opaque parameters and internals.\n",
    "\n",
    "The LCEL implementations can be slightly more verbose, but there are significant benefits in transparency and customizability.\n",
    "\n",
    "In this guide we review LCEL implementations of common legacy abstractions. Where appropriate, we link out to separate guides with more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain-core langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3621b62-a037-42b8-8faa-59575608bb8b",
   "metadata": {},
   "source": [
    "## LLMChain\n",
    "\n",
    "[`LLMChain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html) combined a prompt template, LLM, and output parser into a class.\n",
    "\n",
    "Some advantages of switching to LCEL to replace it are:\n",
    "\n",
    "- Clarity around contents and parameters. The legacy `LLMChain` contains a default output parser and other options.\n",
    "- Easier streaming. `LLMChain` only supports streaming via callbacks.\n",
    "- Easier access to raw model outputs if desired. `LLMChain` only exposes these via a parameter or via callback.\n",
    "\n",
    "import { ColumnContainer, Column } from \"@theme/Columns\";\n",
    "\n",
    "<ColumnContainer>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### Legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e628905c-430e-4e4a-9d7c-c91d2f42052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacoblee/.pyenv/versions/3.10.5/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/Users/jacoblee/.pyenv/versions/3.10.5/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adjective': 'funny',\n",
       " 'text': \"Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
    "\n",
    "chain(\"funny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3b527-c09e-4c77-9711-c3cc4506cd95",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### LCEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2a7cf8-1bc7-405c-bb0d-f2ab2ba3b6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the scarecrow win an award? Because he was outstanding in his field!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"user\", \"Tell me a {adjective} joke\")],\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI() | StrOutputParser()\n",
    "\n",
    "chain.invoke({ \"adjective\": \"funny\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b0513-77b8-4371-a20e-3e487cec7e7f",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "</ColumnContainer>\n",
    "\n",
    "Note that `LLMChain` by default returns a `dict` containing both the input and the output. If this behavior is desired, we can replicate it using another LCEL primitive, [`RunnablePassthrough`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529206c5-abbe-4213-9e6c-3b8586c8000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adjective': 'funny',\n",
       " 'text': 'Why was the math book sad?\\n\\nBecause it had too many problems!'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "outer_chain = RunnablePassthrough().assign(text=chain)\n",
    "\n",
    "outer_chain.invoke({ \"adjective\": \"funny\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2e26c-2854-4971-9c2b-613450993921",
   "metadata": {},
   "source": [
    "See [this tutorial](/docs/tutorials/llm_chain) for more detail on building with prompt templates, LLMs, and output parsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df631d-5121-4918-94aa-b88acce9b769",
   "metadata": {},
   "source": [
    "## ConversationChain\n",
    "\n",
    "[`ConversationChain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversation.base.ConversationChain.html) incorporates a memory of previous messages to sustain a stateful conversation.\n",
    "\n",
    "Some advantages of switching to LCEL to replace it are:\n",
    "\n",
    "- Innate support for threads/separate sessions. To make this work with `ConversationChain`, you'd need to instantiate a separate memory class outside the chain.\n",
    "- More explicit parameters. `ConversationChain` contains a hidden default prompt.\n",
    "- Streaming support. `ConversationChain` only supports streaming via callbacks.\n",
    "\n",
    "`RunnableWithMessageHistory` implements sessions via configuration parameters. It should be instantiated with a callable that returns a [chat message history](https://api.python.langchain.com/en/latest/chat_history/langchain_core.chat_history.BaseChatMessageHistory.html). By default, it expects this function to take a single argument `session_id`.\n",
    "\n",
    "<ColumnContainer>\n",
    "<Column>\n",
    "\n",
    "#### Legacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2cc6dc-d70a-4c13-9258-452f14290da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'how are you?',\n",
       " 'history': '',\n",
       " 'response': \"I'm doing well, thank you for asking! I've been busy analyzing data and learning new information. How about you, how are you doing today?\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chain = ConversationChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "chain({\"input\": \"how are you?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36b0e-c7dc-4130-a51b-189d4b756c7f",
   "metadata": {},
   "source": [
    "</Column>\n",
    "\n",
    "<Column>\n",
    "\n",
    "#### LCEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173e1a9c-2a18-4669-b0de-136f39197786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr matey, I be sailin' the high seas and feelin' like a true swashbuckler! Aye, I be doin' just fine, thank ye for askin'. What be ye needin' from this ol' pirate?\", response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 31, 'total_tokens': 86}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-01f30388-cc24-4977-9f64-73f5d38a60c8-0', usage_metadata={'input_tokens': 31, 'output_tokens': 55, 'total_tokens': 86})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a pirate. Answer the following questions as best you can:\\n{input}\"\n",
    ")\n",
    "history = InMemoryChatMessageHistory()\n",
    "chain = RunnableWithMessageHistory(\n",
    "    prompt | ChatOpenAI(),\n",
    "    lambda x: history\n",
    ")\n",
    "\n",
    "chain.invoke(\n",
    "    {\"input\": \"how are you?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"42\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386ce6-895e-442c-88f3-7bec0ab9f401",
   "metadata": {},
   "source": [
    "\n",
    "</Column>\n",
    "</ColumnContainer>\n",
    "\n",
    "The above example uses the same `history` for all sessions. The example below shows how to use a different chat history for each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e05994f-1fbc-4699-bf2e-62cb0e4deeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Arrr matey! What be ye wantin' from this ole pirate?\", response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 29, 'total_tokens': 45}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0db337ee-c163-42e7-b0f0-cc8668551f03-0', usage_metadata={'input_tokens': 29, 'output_tokens': 16, 'total_tokens': 45})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chain = RunnableWithMessageHistory(prompt | ChatOpenAI(), get_session_history)\n",
    "\n",
    "chain.invoke(\"Hello!\", config={\"configurable\": {\"session_id\": \"abc123\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ebecb",
   "metadata": {},
   "source": [
    "See [this tutorial](/docs/tutorials/chatbot) for a more end-to-end guide on building with [`RunnableWithMessageHistory`](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html).\n",
    "\n",
    "## ConversationRetrievalChain\n",
    "\n",
    "The [`ConversationalRetrievalChain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\n",
    "\n",
    "Some advantages of switching to LCEL to replace it are:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
