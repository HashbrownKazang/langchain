{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "---\n",
    "sidebar_label: Yuan2.0\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Yuan2.0\n",
    "\n",
    "This notebook shows how to use [YUAN2 API](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/inference_server.md) in LangChain with the langchain.chat_models.ChatYuan2.\n",
    "\n",
    "[*Yuan2.0*](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/README-EN.md) is a new generation Fundamental Large Language Model developed by IEIT System. We have published all three models, Yuan 2.0-102B, Yuan 2.0-51B, and Yuan 2.0-2B. And we provide relevant scripts for pretraining, fine-tuning, and inference services for other developers. Yuan2.0 is based on Yuan1.0, utilizing a wider range of high-quality pre training data and instruction fine-tuning datasets to enhance the model's understanding of semantics, mathematics, reasoning, code, knowledge, and other aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting started\n",
    "### Installation\n",
    "First, Yuan2.0 provided an OpenAI compatible API, and we integrate ChatYuan2 into langchain chat model by using OpenAI client.\n",
    "Therefore, ensure the openai package is installed in your Python environment. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet openai langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Importing the Required Modules\n",
    "After installation, import the necessary modules to your Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatYuan2\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setting Up Your API server\n",
    "Setting up your OpenAI compatible API server following [yuan2 openai api server](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/docs/Yuan2_fastchat.md).\n",
    "If you deployed api server locally, you can simply set `yuan2_api_key=\"EMPTY\"` or anything you want.\n",
    "Just make sure, the `yuan2_api_base` is set correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "yuan2_api_key = \"your_api_key\"\n",
    "yuan2_api_base = \"http://127.0.0.1:8001/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initialize the ChatYuan2 Model\n",
    "Here's how to initialize the chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chat = ChatYuan2(\n",
    "    yuan2_api_base=\"http://127.0.0.1:8001/v1\",\n",
    "    temperature=1.0,\n",
    "    model_name=\"yuan2\",\n",
    "    max_retries=3,\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic Usage\n",
    "Invoke the model with system and human messages like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"\u4f60\u662f\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u3002\"),\n",
    "    HumanMessage(content=\"\u4f60\u597d\uff0c\u4f60\u662f\u8c01\uff1f\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(chat.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Basic Usage with streaming\n",
    "For continuous interaction, use the streaming feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "chat = ChatYuan2(\n",
    "    yuan2_api_base=\"http://127.0.0.1:8001/v1\",\n",
    "    temperature=1.0,\n",
    "    model_name=\"yuan2\",\n",
    "    max_retries=3,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"\u4f60\u662f\u4e2a\u65c5\u6e38\u5c0f\u52a9\u624b\u3002\"),\n",
    "    HumanMessage(content=\"\u7ed9\u6211\u4ecb\u7ecd\u4e00\u4e0b\u5317\u4eac\u6709\u54ea\u4e9b\u597d\u73a9\u7684\u3002\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Advanced Features\n",
    "### Usage with async calls\n",
    "\n",
    "Invoke the model with non-blocking calls, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "async def basic_agenerate():\n",
    "    chat = ChatYuan2(\n",
    "        yuan2_api_base=\"http://127.0.0.1:8001/v1\",\n",
    "        temperature=1.0,\n",
    "        model_name=\"yuan2\",\n",
    "        max_retries=3,\n",
    "    )\n",
    "    messages = [\n",
    "        [\n",
    "            SystemMessage(content=\"\u4f60\u662f\u4e2a\u65c5\u6e38\u5c0f\u52a9\u624b\u3002\"),\n",
    "            HumanMessage(content=\"\u7ed9\u6211\u4ecb\u7ecd\u4e00\u4e0b\u5317\u4eac\u6709\u54ea\u4e9b\u597d\u73a9\u7684\u3002\"),\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    result = await chat.agenerate(messages)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "asyncio.run(basic_agenerate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Usage with prompt template\n",
    "\n",
    "Invoke the model with non-blocking calls and used chat template like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "async def ainvoke_with_prompt_template():\n",
    "    from langchain_core.prompts.chat import (\n",
    "        ChatPromptTemplate,\n",
    "    )\n",
    "\n",
    "    chat = ChatYuan2(\n",
    "        yuan2_api_base=\"http://127.0.0.1:8001/v1\",\n",
    "        temperature=1.0,\n",
    "        model_name=\"yuan2\",\n",
    "        max_retries=3,\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"\u4f60\u662f\u4e00\u4e2a\u8bd7\u4eba\uff0c\u64c5\u957f\u5199\u8bd7\u3002\"),\n",
    "            (\"human\", \"\u7ed9\u6211\u5199\u9996\u8bd7\uff0c\u4e3b\u9898\u662f{theme}\u3002\"),\n",
    "        ]\n",
    "    )\n",
    "    chain = prompt | chat\n",
    "    result = await chain.ainvoke({\"theme\": \"\u660e\u6708\"})\n",
    "    print(f\"type(result): {type(result)}; {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "asyncio.run(ainvoke_with_prompt_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Usage with async calls in streaming\n",
    "For non-blocking calls with streaming output, use the astream method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "async def basic_astream():\n",
    "    chat = ChatYuan2(\n",
    "        yuan2_api_base=\"http://127.0.0.1:8001/v1\",\n",
    "        temperature=1.0,\n",
    "        model_name=\"yuan2\",\n",
    "        max_retries=3,\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(content=\"\u4f60\u662f\u4e2a\u65c5\u6e38\u5c0f\u52a9\u624b\u3002\"),\n",
    "        HumanMessage(content=\"\u7ed9\u6211\u4ecb\u7ecd\u4e00\u4e0b\u5317\u4eac\u6709\u54ea\u4e9b\u597d\u73a9\u7684\u3002\"),\n",
    "    ]\n",
    "    result = chat.astream(messages)\n",
    "    async for chunk in result:\n",
    "        print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "asyncio.run(basic_astream())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}