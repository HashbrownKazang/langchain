# NVIDIA

>[Nvidia Corporation (Wikipedia)](https://en.wikipedia.org/wiki/Nvidia) is an American multinational 
> corporation and technology company. It is a software and fabless company which 
> designs and supplies graphics processing units (GPUs), application programming interfaces (APIs) 
> for data science and high-performance computing as well as system on 
> a chip units (SoCs) for the mobile computing and automotive market. 
> `Nvidia` is also a dominant supplier of artificial intelligence (AI) hardware and software.

## NVIDIA AI Foundation Endpoints

>`NVIDIA` provides an integration package for `LangChain`: `langchain-nvidia-ai-endpoints`.

> [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/) give users easy access to NVIDIA hosted API endpoints for 
> NVIDIA AI Foundation Models like `Mixtral 8x7B`, `Llama 2`, `Stable Diffusion`, etc. These models, 
> hosted on the [NVIDIA API catalog](https://build.nvidia.com/), are optimized, tested, and hosted on 
> the NVIDIA AI platform, making them fast and easy to evaluate, further customize, 
> and seamlessly run at peak performance on any accelerated stack.
> 
> With [NVIDIA AI Foundation Endpoints](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), you can get quick results from a fully 
> accelerated stack running on [NVIDIA DGX Cloud](https://www.nvidia.com/en-us/data-center/dgx-cloud/). Once customized, these 
> models can be deployed anywhere with enterprise-grade security, stability, 
> and support using [NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/).

A selection of NVIDIA AI Foundation models is supported directly in LangChain with familiar APIs.

The supported models can be found [in build.nvidia.com](https://build.nvidia.com/).

These models can be accessed via the [`langchain-nvidia-ai-endpoints`](https://pypi.org/project/langchain-nvidia-ai-endpoints/) 
package, as shown below.

### Setting up

1. Create a free account with [NVIDIA](https://build.nvidia.com/), which hosts NVIDIA AI Foundation models

2. Click on your model of choice

3. Under `Input` select the `Python` tab, and click `Get API Key`. Then click `Generate Key`.

4. Copy and save the generated key as `NVIDIA_API_KEY`. From there, you should have access to the endpoints.

```bash
export NVIDIA_API_KEY=nvapi-XXXXXXXXXXXXXXXXXXXXXXXXXX
```

- Install a package:

```bash
pip install -U langchain-nvidia-ai-endpoints
```

### LLMs


```python
from langchain_nvidia_ai_endpoints.llm import NVIDIA
```

### Chat models

See a [usage example](/docs/integrations/chat/nvidia_ai_endpoints).

```python
from langchain_nvidia_ai_endpoints import ChatNVIDIA
```

### Embedding models

See a [usage example](/docs/integrations/text_embedding/nvidia_ai_endpoints).

```python
from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
```

### Tools

```python
from langchain_nvidia_ai_endpoints.tools import ServerToolsMixin
```

### Callbacks

```python
from langchain_nvidia_ai_endpoints.callbacks import UsageCallbackHandler
```

### Image Generation models

```python
from langchain_nvidia_ai_endpoints.image_gen import ImageGenNVIDIA
```

### Reranker

```python
from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank
from langchain_nvidia_ai_endpoints.reranking import Ranking
```

## NVIDIA Triton™ Inference Server

>`NVIDIA` provides another integration package for `LangChain`: `langchain-nvidia-trt`.

>[NVIDIA Triton™ Inference Server](https://developer.nvidia.com/triton-inference-server), 
> part of the `NVIDIA AI` platform and available with `NVIDIA AI Enterprise`, is open-source 
> software that standardizes AI model deployment and execution across every workload.

### Setting up

- See [installation instructions](https://developer.nvidia.com/triton-inference-server).
- Install packages:

```bash
pip install -U tritonclient langchain-nvidia-trt
```

### LLMs


```python
from langchain_nvidia_trt.llms import TritonTensorRTLLM
```

## Third-party integrations

### Embedding models

#### NVIDIA NeMo embeddings

See a [usage example](/docs/integrations/text_embedding/nemo).

```python
from langchain_community.embeddings import NeMoEmbeddings
```
